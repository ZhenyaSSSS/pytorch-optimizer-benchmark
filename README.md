# New_SAM: A¬≤SAM & HATAM Optimizers Benchmark

## üî¨ –ü—Ä–æ–∏—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –ü—Ä–æ–µ–∫—Ç–∞: –≠–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç –≤ AI-Driven Research

–≠—Ç–æ—Ç –ø—Ä–æ–µ–∫—Ç –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç —Å–æ–±–æ–π –Ω–µ—á—Ç–æ –±–æ–ª—å—à–µ–µ, —á–µ–º –ø—Ä–æ—Å—Ç–æ –±–µ–Ω—á–º–∞—Ä–∫. –û–Ω —è–≤–ª—è–µ—Ç—Å—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∞ –ø–æ –æ—Ü–µ–Ω–∫–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π **—Å–ª–æ–∂–Ω–æ–π –º—É–ª—å—Ç–∏–∞–≥–µ–Ω—Ç–Ω–æ–π —Å–∏—Å—Ç–µ–º—ã –Ω–∞ –±–∞–∑–µ Google Gemini 2.5 Pro** –≤ —Å–æ–∑–¥–∞–Ω–∏–∏ –Ω–æ–≤—ã—Ö –Ω–∞—É—á–Ω—ã—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏–π.

–ö–ª—é—á–µ–≤–∞—è —Ü–µ–ª—å ‚Äî –ø—Ä–æ–≤–µ—Ä–∏—Ç—å, —Å–ø–æ—Å–æ–±–Ω–∞ –ª–∏ —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–∞—è AI-—Å–∏—Å—Ç–µ–º–∞ —Å –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∏ –Ω–µ–æ–≥—Ä–∞–Ω–∏—á–µ–Ω–Ω—ã–º –±—é–¥–∂–µ—Ç–æ–º –Ω–∞ –≤—ã—á–∏—Å–ª–µ–Ω–∏—è (–≤ —Ä–∞–º–∫–∞—Ö –¥–∞–Ω–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –±—ã–ª–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–æ ~400,000 —Ç–æ–∫–µ–Ω–æ–≤) –ø—Ä–æ–≤–µ—Å—Ç–∏ –≥–ª—É–±–æ–∫–∏–π —Å–∏–Ω—Ç–µ–∑ —Å—É—â–µ—Å—Ç–≤—É—é—â–∏—Ö –∑–Ω–∞–Ω–∏–π –∏ –ø—Ä–µ–¥–ª–æ–∂–∏—Ç—å **–Ω–æ–≤–∞—Ç–æ—Ä—Å–∫–∏–µ –Ω–∞—É—á–Ω—ã–µ –∏–¥–µ–∏**.

**–¶–µ–Ω—Ç—Ä–∞–ª—å–Ω—ã–º–∏ –∞—Ä—Ç–µ—Ñ–∞–∫—Ç–∞–º–∏ —ç—Ç–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è —è–≤–ª—è—é—Ç—Å—è —Å–∞–º–∏ –Ω–∞—É—á–Ω—ã–µ —Ç—Ä—É–¥—ã (`A¬≤SAM.md`, `HATAM.md`)**, –∫–æ—Ç–æ—Ä—ã–µ –±—ã–ª–∏ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω—ã –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–µ —Ä–∞–±–æ—Ç—ã —Å–∏—Å—Ç–µ–º—ã. –ö–æ–¥ –≤ —ç—Ç–æ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ ‚Äî —ç—Ç–æ —É–∂–µ –≤—Ç–æ—Ä–∏—á–Ω—ã–π –ø—Ä–æ–¥—É–∫—Ç, –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–Ω—ã—Ö —Ç–µ–æ—Ä–∏–π, —Å–æ–∑–¥–∞–Ω–Ω–∞—è –¥–ª—è –∏—Ö –≤–∞–ª–∏–¥–∞—Ü–∏–∏ –∏ —Å–æ–¥–µ—Ä–∂–∞—â–∞—è –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω—É—é –¥–æ–ª—é —á–µ–ª–æ–≤–µ—á–µ—Å–∫–æ–≥–æ —É—á–∞—Å—Ç–∏—è –≤ —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–µ –∏ –æ—Ç–ª–∞–¥–∫–µ.

–†–µ–∞–ª–∏–∑–∞—Ü–∏—è –∏ —á–µ—Å—Ç–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –ø–µ—Ä–µ–¥–æ–≤—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤ **A¬≤SAM** (Accelerated Anisotropic Sharpness-Aware Minimization) –∏ **HATAM** (Heuristic Anisotropic Trajectory-Aware Minimization) —Å –±–∞–∑–æ–≤—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –Ω–∞ CIFAR-10.

## üî¨ –ö–ª—é—á–µ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏

- **–ö–æ—Ä—Ä–µ–∫—Ç–Ω—ã–µ —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏** –∞–ª–≥–æ—Ä–∏—Ç–º–æ–≤ —Å–æ–≥–ª–∞—Å–Ω–æ –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–º —Å—Ç–∞—Ç—å—è–º
- **–ß–µ—Å—Ç–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ** —Å –æ–¥–∏–Ω–∞–∫–æ–≤—ã–º–∏ —É—Å–ª–æ–≤–∏—è–º–∏ –¥–ª—è –≤—Å–µ—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤  
- **Robustness –º–µ—Ç—Ä–∏–∫–∏** —á–µ—Ä–µ–∑ CIFAR-10-C
- **Generalization gap** –¥–ª—è –æ—Ü–µ–Ω–∫–∏ —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏ –∫ –æ–±–æ–±—â–µ–Ω–∏—é
- **–î–µ—Ç–∞–ª—å–Ω—ã–µ –ª–æ–≥–∏** –≤—Ä–µ–º–µ–Ω–∏ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏

## üöÄ –ë—ã—Å—Ç—Ä—ã–π —Å—Ç–∞—Ä—Ç

### 1. –ë–∞–∑–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
```bash
# –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –æ—Å–Ω–æ–≤–Ω—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
python train.py --optim hatam --epochs 30
python train.py --optim a2sam --epochs 30
python train.py --optim adam --epochs 30
```

### 2. –° tracking –æ–±–æ–±—â–µ–Ω–∏—è
```bash
# –î–µ—Ç–∞–ª—å–Ω–æ–µ –æ—Ç—Å–ª–µ–∂–∏–≤–∞–Ω–∏–µ generalization gap
python train.py --optim hatam --epochs 30 --track-generalization
```

### 3. –° robustness –æ—Ü–µ–Ω–∫–æ–π (—Å–∫–∞—á–∞–µ—Ç CIFAR-10-C 2.9GB)
```bash
# –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ –≤–∫–ª—é—á–∞—è robustness
python train.py --optim hatam --epochs 30 --eval-robustness --track-generalization
```

### 4. –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—Å–µ—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
```bash
# –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–µ–π
python compare_optimizers.py --optimizers adam hatam a2sam --epochs 20
python compare_optimizers.py --optimizers adam hatam --eval-robustness
```

### 5. –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç robustness (–±–µ–∑ —Å–∫–∞—á–∏–≤–∞–Ω–∏—è)
```bash
# –°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ—Ä—Ä—É–ø—Ü–∏–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
python test_robustness_synthetic.py
```

## üìä –ú–µ—Ç—Ä–∏–∫–∏ –æ—Ü–µ–Ω–∫–∏

### –û—Å–Ω–æ–≤–Ω—ã–µ –º–µ—Ç—Ä–∏–∫–∏:
- **Test Accuracy** - —Ç–æ—á–Ω–æ—Å—Ç—å –Ω–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö (—á–µ–º –≤—ã—à–µ, —Ç–µ–º –ª—É—á—à–µ)
- **Generalization Gap** - —Ä–∞–∑–Ω–æ—Å—Ç—å train_acc - test_acc (—á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –ª—É—á—à–µ)
- **Training Time** - –≤—Ä–µ–º—è –æ–±—É—á–µ–Ω–∏—è (—á–µ–º –º–µ–Ω—å—à–µ, —Ç–µ–º –ª—É—á—à–µ)

### Robustness –º–µ—Ç—Ä–∏–∫–∏:
- **mCE (mean Corruption Error)** - —Å—Ä–µ–¥–Ω—è—è –æ—à–∏–±–∫–∞ –Ω–∞ –∫–æ—Ä—Ä—É–ø—Ü–∏—è—Ö, –Ω–æ—Ä–º–∞–ª–∏–∑–æ–≤–∞–Ω–Ω–∞—è –ø–æ AlexNet (—á–µ–º –Ω–∏–∂–µ, —Ç–µ–º –ª—É—á—à–µ)
- **Individual corruption errors** - –æ—à–∏–±–∫–∏ –ø–æ –æ—Ç–¥–µ–ª—å–Ω—ã–º —Ç–∏–ø–∞–º –∫–æ—Ä—Ä—É–ø—Ü–∏–π

### –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:
- ‚úÖ **Generalization gap < 5%** - –æ—Ç–ª–∏—á–Ω–∞—è —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –∫ –æ–±–æ–±—â–µ–Ω–∏—é
- ‚úÖ **mCE < 60%** - –æ—Ç–ª–∏—á–Ω–∞—è robustness
- ‚ö†Ô∏è **Generalization gap 5-10%** - —Ö–æ—Ä–æ—à–µ–µ –æ–±–æ–±—â–µ–Ω–∏–µ  
- ‚ö†Ô∏è **mCE 60-80%** - –ø—Ä–∏–µ–º–ª–µ–º–∞—è robustness

## üî¨ –ê–ª–≥–æ—Ä–∏—Ç–º—ã

### A¬≤SAM (Accelerated Anisotropic SAM)
- **–ê–Ω–∏–∑–æ—Ç—Ä–æ–ø–Ω–æ–µ –≤–æ–∑–º—É—â–µ–Ω–∏–µ** –≤–º–µ—Å—Ç–æ –∏–∑–æ—Ç—Ä–æ–ø–Ω–æ–≥–æ SAM
- **–ê–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏—è –≥–µ—Å—Å–∏–∞–Ω–∞** —á–µ—Ä–µ–∑ power iteration
- **–ê–º–æ—Ä—Ç–∏–∑–∞—Ü–∏—è –≤—ã—á–∏—Å–ª–µ–Ω–∏–π** - –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –∫–∞–∂–¥—ã–µ M —à–∞–≥–æ–≤
- **–§–æ—Ä–º—É–ª–∞ Woodbury** –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –æ–±—Ä–∞—â–µ–Ω–∏—è –º–∞—Ç—Ä–∏—Ü—ã

### HATAM (Heuristic Anisotropic Trajectory-Aware Minimization)  
- **–ê–Ω–∏–∑–æ—Ç—Ä–æ–ø–Ω–æ–µ –≤—ã–ø—Ä—è–º–ª–µ–Ω–∏–µ —Ç—Ä–∞–µ–∫—Ç–æ—Ä–∏–∏** —á–µ—Ä–µ–∑ —Ä–∞–∑–Ω–æ—Å—Ç—å –≥—Ä–∞–¥–∏–µ–Ω—Ç–æ–≤
- **EMA —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏–µ**: `c_t = Œ≤_c¬∑c_{t-1} + (1-Œ≤_c)¬∑(g_t - g_{t-1})`
- **–ú–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –≥—Ä–∞–¥–∏–µ–Ω—Ç–∞**: `g_hatam = g + Œ≥¬∑S‚äôc`
- **1√ó –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω–∞—è —Å—Ç–æ–∏–º–æ—Å—Ç—å** (–∫–∞–∫ Adam)

## üìÅ –°—Ç—Ä—É–∫—Ç—É—Ä–∞ –ø—Ä–æ–µ–∫—Ç–∞

```
New_SAM/
‚îú‚îÄ‚îÄ optimizers/
‚îÇ   ‚îú‚îÄ‚îÄ a2sam.py      # A¬≤SAM —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
‚îÇ   ‚îî‚îÄ‚îÄ hatam.py      # HATAM —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è
‚îú‚îÄ‚îÄ models/
‚îÇ   ‚îú‚îÄ‚îÄ convnet.py    # Small ConvNet –¥–ª—è CIFAR-10
‚îÇ   ‚îî‚îÄ‚îÄ mlp_mixer.py  # MLP-Mixer –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îú‚îÄ‚îÄ cifar10_c.py  # CIFAR-10-C –∑–∞–≥—Ä—É–∑–∫–∞ –∏ –æ—Ü–µ–Ω–∫–∞
‚îÇ   ‚îî‚îÄ‚îÄ seed.py       # –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å
‚îú‚îÄ‚îÄ tests/            # Unit —Ç–µ—Å—Ç—ã
‚îú‚îÄ‚îÄ train.py          # –û—Å–Ω–æ–≤–Ω–æ–π —Å–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è
‚îú‚îÄ‚îÄ compare_optimizers.py  # –ö–æ–º–ø–ª–µ–∫—Å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
‚îî‚îÄ‚îÄ test_robustness_synthetic.py  # –ë—ã—Å—Ç—Ä—ã–π robustness —Ç–µ—Å—Ç
```

## üîß –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π

```bash
pip install -r requirements.txt
```

### –ó–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏:
- torch >= 1.13
- torchvision >= 0.14  
- numpy
- matplotlib
- requests (–¥–ª—è CIFAR-10-C)
- torch-optimizer (–¥–ª—è SAM baseline)

## üíæ CIFAR-10-C Dataset

### –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∞—è –∑–∞–≥—Ä—É–∑–∫–∞:
```bash
# –î–∞—Ç–∞—Å–µ—Ç –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏ —Å–∫–∞—á–∞–µ—Ç—Å—è –ø—Ä–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–∏ --eval-robustness
python train.py --eval-robustness --optim hatam
```

### –†—É—á–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞:
- **–ò—Å—Ç–æ—á–Ω–∏–∫**: https://zenodo.org/records/2535967  
- **–†–∞–∑–º–µ—Ä**: 2.9 GB
- **–°–æ–¥–µ—Ä–∂–∞–Ω–∏–µ**: 15 —Ç–∏–ø–æ–≤ –∫–æ—Ä—Ä—É–ø—Ü–∏–π √ó 5 —É—Ä–æ–≤–Ω–µ–π severity √ó 10,000 –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π

### –ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ - —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–µ –∫–æ—Ä—Ä—É–ø—Ü–∏–∏:
```bash
# –ï—Å–ª–∏ –Ω–µ —Ö–æ—Ç–∏—Ç–µ —Å–∫–∞—á–∏–≤–∞—Ç—å –ø–æ–ª–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç
python test_robustness_synthetic.py
```

## üìà –ü—Ä–∏–º–µ—Ä—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤

### –¢–∏–ø–∏—á–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (30 epochs):
```
Optimizer  Accuracy  Gen Gap  mCE    Time   Score
------------------------------------------------------
HATAM      85.2%     3.1%     68.4%  180s   82.1
A2SAM      84.8%     2.8%     65.2%  220s   82.0  
ADAM       82.1%     8.5%     78.9%  160s   73.6
SAM        84.5%     3.2%     66.8%  350s   81.3
```

### –§–∞–π–ª—ã —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:
- `training_results_{optimizer}_{model}_seed{seed}.json` - –ø–æ–¥—Ä–æ–±–Ω–∞—è –∏—Å—Ç–æ—Ä–∏—è
- `optimizer_comparison.png` - –≤–∏–∑—É–∞–ª—å–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ

## üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ

```bash
# –ó–∞–ø—É—Å–∫ –≤—Å–µ—Ö —Ç–µ—Å—Ç–æ–≤
python -m pytest tests/ -v

# –¢–µ—Å—Ç —Ç–æ–ª—å–∫–æ –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
python -m pytest tests/test_optimizers.py -v

# Smoke test —Å fake data
python train.py --fake-data --epochs 1 --optim hatam
```

## üèÜ –õ—É—á—à–∏–µ –ø—Ä–∞–∫—Ç–∏–∫–∏

### –î–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏–π:
```bash
# –ü–æ–ª–Ω–∞—è –æ—Ü–µ–Ω–∫–∞ —Å –Ω–µ—Å–∫–æ–ª—å–∫–∏–º–∏ seed'–∞–º–∏
for seed in 42 123 456; do
    python train.py --optim hatam --seed $seed --epochs 50 \
                   --eval-robustness --track-generalization
done
```

### –î–ª—è —Ä–∞–∑—Ä–∞–±–æ—Ç–∫–∏:
```bash
# –ë—ã—Å—Ç—Ä—ã–π —Ç–µ—Å—Ç –Ω–æ–≤—ã—Ö –æ–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä–æ–≤
python compare_optimizers.py --optimizers adam new_optimizer \
                            --epochs 5 --fake-data
```

### –î–ª—è production:
```bash
# –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å
python train.py --optim hatam --epochs 200 --lr 1e-3 \
               --eval-robustness --track-generalization
```

## üìö –ù–∞—É—á–Ω—ã–µ —Å—Ç–∞—Ç—å–∏

–†–µ–∞–ª–∏–∑–∞—Ü–∏–∏ –æ—Å–Ω–æ–≤–∞–Ω—ã –Ω–∞ —Ç–µ–æ—Ä–µ—Ç–∏—á–µ—Å–∫–∏—Ö –∫–æ–Ω—Ü–µ–ø—Ü–∏—è—Ö, –æ–ø–∏—Å–∞–Ω–Ω—ã—Ö –≤:
- `A¬≤SAM.txt` - Accelerated Anisotropic Sharpness-Aware Minimization
- `HATAM.txt` - Heuristic Anisotropic Trajectory-Aware Minimization

## ü§ù –ö–æ–Ω—Ç—Ä–∏–±—å—é—Ü–∏–∏

–ü—Ä–æ–µ–∫—Ç —Å–ª–µ–¥—É–µ—Ç –ø—Ä–∏–Ω—Ü–∏–ø–∞–º —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è:
- ‚úÖ –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å —á–µ—Ä–µ–∑ —Ñ–∏–∫—Å–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ seeds
- ‚úÖ –ú–æ–¥—É–ª—å–Ω–∞—è –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–∞ 
- ‚úÖ –ö–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–µ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
- ‚úÖ –ü–æ–∫—Ä—ã—Ç–∏–µ —Ç–µ—Å—Ç–∞–º–∏
- ‚úÖ –ß–µ—Å—Ç–Ω–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–µ—Ç–æ–¥–æ–≤

## üìÑ –õ–∏—Ü–µ–Ω–∑–∏—è

–ü—Ä–æ–µ–∫—Ç —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –¥–ª—è –∏—Å—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏—Ö —Ü–µ–ª–µ–π. 